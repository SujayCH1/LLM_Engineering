{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Chat Model or LLM + Embeddign generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama # import chat model not llm, both are different\n",
    "from langchain_core.messages import HumanMessage,SystemMessage # allows to define which is system message (context for human message) and which is human message\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OllamaEmbeddings # embeddings models help convert data into vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =  ChatOllama(model='llama3.2') # load the chat model\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\") # load the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29047149419784546, 0.7891371846199036, -2.741931676864624, -1.7968902587890625, 0.7434597015380859]\n"
     ]
    }
   ],
   "source": [
    "vector = embedding_model.embed_query(\"What is LangChain?\") # embedding process / function\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  messages = [ \n",
    "    SystemMessage(\"Translate the following from English into Hindi. Just give the translation straightforward Noting else\"),\n",
    "    HumanMessage(\"Context\"),\n",
    "] # define human and system messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "संदर्भ\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages) # call the chat model.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to make vector databases using chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb # import the chromadb library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\") # store the db on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\") # create a collection. Collection are like sql tables but for vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: doc1\n",
      "Insert of existing embedding ID: doc2\n",
      "Add of existing embedding ID: doc1\n",
      "Add of existing embedding ID: doc2\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    ids=[\"doc1\", \"doc2\"],\n",
    "    metadatas=[{\"source\": \"article\"}, {\"source\": \"blog\"}],\n",
    "    documents=[\"LangChain is a framework for LLM apps.\", \"Vector databases store embeddings efficiently.\"]\n",
    ") # here we add some sample docs aling with metadata to pur collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc2']], 'embeddings': None, 'documents': [['Vector databases store embeddings efficiently.']], 'uris': None, 'data': None, 'metadatas': [[{'source': 'blog'}]], 'distances': [[1.1538174968333408]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"What is databases\"],\n",
    "    n_results=1\n",
    ") # Now we query and pritn teh results.\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ChromaDB with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma # immport chroma from langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model2 = OllamaEmbeddings(model=\"nomic-embed-text\") # make and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma(persist_directory=\"./chroma_db2\", embedding_function=embedding_model) # create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['96bd2af3-79a7-466c-95ca-c092c8b8f70a',\n",
       " 'dd5b8129-7293-467a-8f02-778a938e8cf9']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"LangChain is a powerful LLM framework.\", \"ChromaDB helps store embeddings.\"]\n",
    "vector_db.add_texts(texts) # add some texts  /docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='ChromaDB helps store embeddings.')]\n"
     ]
    }
   ],
   "source": [
    "query = \"what is used to save vector data ?\"\n",
    "results = vector_db.similarity_search(query, k=1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Storing Large Documents (Text Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # import the text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain is a powerful tool for building applications with LLMs. It provides integrations with vector databases, retrieval mechanisms, and tools like ChromaDB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LangChain is a powerful tool for building', 'building applications with LLMs. It provides', 'provides integrations with vector databases,', 'retrieval mechanisms, and tools like ChromaDB.']\n"
     ]
    }
   ],
   "source": [
    "# Split text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using retrivers - basic, max marginal relevance, multiquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. I can provide some general information about vector databases, though.\n",
      "\n",
      "A vector database is a type of data storage system designed to efficiently manage and query large collections of dense vectors in high-dimensional spaces. Vector databases are often used for applications such as recommendation systems, search, and clustering.\n",
      "\n",
      "In the context of ChromaDB, it's likely that vector databases are being utilized to store and manage embeddings, which can be used for various purposes like facial recognition, object detection, or natural language processing.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load embeddings & ChromaDB\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vector_db = Chroma(persist_directory=\"./chroma_db2\", embedding_function=embedding_model)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})  # Fetch top 2 relevant docs\n",
    "\n",
    "# Load LLM\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# Create a Retrieval-Augmented QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is a vector database?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(response)  # This should use retrieved docs + LLM to generate an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
